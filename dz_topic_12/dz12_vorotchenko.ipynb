{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8b847c0e-7fc6-4ed3-b08c-44b068e3fd00",
    "_uuid": "5ca223fa-705b-4115-89c8-9da6f7f9ab9f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "65893683-99bd-4414-93a9-a9edce4ac115",
    "_uuid": "64ff0b7b-a376-4ae8-9b56-ecf55e0820d8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dz_12/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments,  DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "import sentencepiece\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "940695be-406e-4e9c-82ef-c7efdecbcaa7",
    "_uuid": "b66757ab-f6bc-49e2-a854-aad031605645",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "57dd2e7f-f3d0-4040-9ee8-58447029709b",
    "_uuid": "bf593aca-fb89-4335-a0d1-56276df67040",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Завантаження та підготовка даних:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "091a9ded-c599-48e8-865e-e0bab923640a",
    "_uuid": "a0b091bf-c745-4095-98be-9cdc1acbc05d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "ds = load_dataset(\"Helsinki-NLP/europarl\", \"en-sk\")\n",
    "\n",
    "# Split Data\n",
    "train_valid_split = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_data = train_valid_split['train']\n",
    "valid_data = train_valid_split['test']\n",
    "\n",
    "# Add these lines to reduce dataset size\n",
    "train_data = train_data.select(range(5000))\n",
    "valid_data = valid_data.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "cd1ed07f-8f9e-4448-9050-012fb64db101",
    "_uuid": "8f27bf03-56c6-449c-a8d1-ca605473b899",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dz_12/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4920.29 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5353.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-sk\")\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    source_texts = [item['en'] for item in examples['translation']]\n",
    "    target_texts = [item['sk'] for item in examples['translation']]\n",
    "    return tokenizer(\n",
    "        source_texts, \n",
    "        text_target=target_texts, \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "train_tokenized = train_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"translation\"]\n",
    ")\n",
    "\n",
    "valid_tokenized = valid_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"translation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "2e0d4161-6a82-418f-91ec-09e9d7b98334",
    "_uuid": "7c501839-23cf-49e5-b17a-4b7ae6d0926b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "# Create Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"Helsinki-NLP/opus-mt-en-sk\")\n",
    "\n",
    "# Create Data Loaders\n",
    "train_dataloader = DataLoader(train_tokenized, shuffle=True, batch_size=16, collate_fn=data_collator)\n",
    "valid_dataloader = DataLoader(valid_tokenized, batch_size=16, collate_fn=data_collator)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51b59b55-68f7-49f2-819a-3c220516e2d5",
    "_uuid": "30672150-8379-43ac-8358-695383748be3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4. Побудова моделі:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "c050abd0-1687-46b5-9e41-ab83b05674d5",
    "_uuid": "8fbf37de-fee1-49b3-8425-9afb7362104e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))  # [batch_size, src_len, emb_dim]\n",
    "        outputs, hidden = self.rnn(embedded)  # outputs: [batch_size, src_len, hid_dim]\n",
    "        # hidden: [n_layers, batch_size, hid_dim]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "ef3aceef-2bf9-4f9b-9277-0549add9d75d",
    "_uuid": "62b00020-c36c-490b-963c-b712f0ea31b6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch_size, hid_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hid_dim]\n",
    "\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hid_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "f9af4a37-6790-423d-9dc3-ef773364614d",
    "_uuid": "6cda6983-b603-46ca-bc59-7bb9b0f84d4d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [n_layers, batch_size, hid_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hid_dim]\n",
    "\n",
    "        input = input.unsqueeze(1)  # [batch_size, 1]\n",
    "        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, emb_dim]\n",
    "\n",
    "        # Use the top layer hidden state for attention\n",
    "        attn_weights = self.attention(hidden[-1, :, :], encoder_outputs)  # [batch_size, src_len]\n",
    "        attn_weights = attn_weights.unsqueeze(1)  # [batch_size, 1, src_len]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # [batch_size, 1, hid_dim]\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, emb_dim + hid_dim]\n",
    "        # Use hidden as is: shape [n_layers, batch_size, hid_dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden)  # output: [batch_size, 1, hid_dim], hidden: [n_layers, batch_size, hid_dim]\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1), embedded.squeeze(1)), dim=1))\n",
    "        # prediction: [batch_size, output_dim]\n",
    "\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0a15ccf2-d027-466d-b858-e5e8f4d72db8",
    "_uuid": "328db969-1bbd-4144-b772-258701bce33c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # Define the start token ID safely\n",
    "        start_token_id = tokenizer.bos_token_id\n",
    "        if start_token_id is None:\n",
    "            start_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        # Initialize the first decoder input token\n",
    "        input = torch.full((batch_size,), start_token_id, dtype=torch.long, device=self.device)\n",
    "    \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size, device=self.device)\n",
    "    \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "    \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "    \n",
    "            # Get the target token at time-step t\n",
    "            next_token = trg[:, t]\n",
    "    \n",
    "            # Replace any -100 indices (padding for labels) with the model's prediction (top1)\n",
    "            next_token = torch.where(next_token == -100, top1, next_token)\n",
    "    \n",
    "            input = next_token if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "2b3b8524-5966-4598-9d63-2e9af69c014d",
    "_uuid": "7050b87f-9d6a-4ad0-b8ec-68f01d19ccb4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(tokenizer.get_vocab())  # Vocabulary size for the source language\n",
    "OUTPUT_DIM = len(tokenizer.get_vocab())  # Vocabulary size for the target language\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Initialize Components\n",
    "attention = Attention(HID_DIM)\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, attention)\n",
    "\n",
    "# Combine into Seq2Seq Model\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dedffcf1-4ba4-4271-baaa-65b26e87811a",
    "_uuid": "69da1739-e198-4f6a-b6cd-33e58d86e1c7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5. Визначення функції втрат та оптимізатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "52cab4b0-b997-4e4e-9058-81ce0a6b7632",
    "_uuid": "4692bae4-8b11-445a-a2be-0abeeb714a58",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88cd4c7d-724c-44ce-9e81-d6cdcc587f4d",
    "_uuid": "b59297df-7696-4ba2-b063-225d17dde02e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6. Навчання моделі:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "d78f91cc-b441-4278-b625-201cf15af485",
    "_uuid": "8ebb0139-9a7b-42e3-bbf5-8e7508368847",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    print(\"Train one epoch\")\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i} batches...\")\n",
    "\n",
    "        src = batch['input_ids'].to(device)       # Source tokens\n",
    "        trg = batch['labels'].to(device)          # Target tokens\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, trg)  # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)  # Exclude <sos> token\n",
    "        trg = trg[:, 1:].reshape(-1)  # Exclude <sos> token\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b3486c08-e443-4f1f-9f6b-e6122604ed0f",
    "_uuid": "72436269-c68e-492b-a01a-7a23ec0f9ca3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, valid_dataloader, optimizer, criterion, device, n_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "\n",
    "        # Training phase\n",
    "        train_loss = train_one_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase (no gradients)\n",
    "        valid_loss = evaluate_model(model, valid_dataloader, criterion, device)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        print(f'Training Loss: {train_loss:.4f} | Validation Loss: {valid_loss:.4f}')\n",
    "    \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "8d75f5d5-1662-4473-a0d7-faa3fa0beeed",
    "_uuid": "fec7c08f-1100-4dc6-b934-3fa230753455",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch['input_ids'].to(device)\n",
    "            trg = batch['labels'].to(device)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)  # Disable teacher forcing for validation\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)  # Exclude <sos> token\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "ef1b57d3-da4e-474a-b18e-2fce96c33b6c",
    "_uuid": "1780c5ed-19eb-4226-87c9-57eac9c99028",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train one epoch\n",
      "Processed 0 batches...\n",
      "Processed 100 batches...\n",
      "Processed 200 batches...\n",
      "Processed 300 batches...\n",
      "Training Loss: nan | Validation Loss: nan\n",
      "Epoch 2/3\n",
      "Train one epoch\n",
      "Processed 0 batches...\n",
      "Processed 100 batches...\n",
      "Processed 200 batches...\n",
      "Processed 300 batches...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m N_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Save Losses for Analysis\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_losses, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: valid_losses}\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, valid_dataloader, optimizer, criterion, device, n_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Validation phase (no gradients)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dz_12/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dz_12/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dz_12/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "N_EPOCHS = 3\n",
    "\n",
    "# Train the Model\n",
    "train_losses, valid_losses = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    "    n_epochs=N_EPOCHS\n",
    ")\n",
    "\n",
    "# Save Losses for Analysis\n",
    "loss_data = {'train_loss': train_losses, 'valid_loss': valid_losses}\n",
    "torch.save(loss_data, 'loss_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d12cca33-db71-4c39-8e0b-e03e6b48abb1",
    "_uuid": "5eb94f2c-1f69-4d86-83e3-b6878237c43a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, valid_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6de85898-516b-4445-a2c2-a344bb4dea17",
    "_uuid": "79dbcd64-bc0b-49c7-ad1f-ff683e7ee885",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:dz_12]",
   "language": "python",
   "name": "conda-env-dz_12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
